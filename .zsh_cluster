#!/bin/zsh
#================================================================
#- IMPLEMENTATION
#-    version         0.0.8
#-    author          Maximilian Oppelt
#-
#================================================================
#  HISTORY
#     2021/01/01 : Maximilian Oppelt : File created.
#     2021/02/11 : Maximilian Oppelt : Add script name as option
#     2021/12/14 : Maximilian Oppelt : Bug fix in project name and Update docker image
#     2021/12/23 : Maximilian Oppelt : New custom image for faster startup
#     2021/12/24 : Maximilian Oppelt : The cluster script will be deprecated 01/2022
#                                      Added initial version for new interface.
#     2022/01/20 : Maximilian Oppelt : Increased shared memory for pytorch dataloader.
#     2022/01/21 : Maximilian Oppelt : Multi node gpu training for pytorch.
#     2022/02/12 : Maximilian Oppelt : Added initialization function for other users.
#     2024/03/19 : Maximilian Oppelt : Updated pytorch docker version.
#     2024/03/20 : Maximilian Oppelt : Project files are now moved to the container
#                                      workspace and deleted after the job.
#
#================================================================

function dlh()
{
    echo "# ===================================================================="
    echo "#  COLLECTION OF LAZY DEEP LEARNER CLUSTER SCRIPT"
    echo "# ===================================================================="
    echo "# These functions simplify the managment (starting/stopping/observing)"
    echo "# of deep learning jobs on the SSE deep learning cluster."
    echo "#"
    echo "#  USAGE"
    echo "# ===================================================================="
    echo "#\tCreating a new job:       \tdlr <SCRIPT>"
    echo "#\tPrinting log of job:      \tdll <JOBNAME>"
    echo "#\tDeleting (old) jobs:      \tdld <JOBNAME>"
    echo "#\tObserving current jobs:   \tdls"
    echo "#\tRemove all jobs:          \tdlda"
    echo "#\tInitialize the env:       \tdlinit"
    echo "# ===================================================================="
} 

function dll() {
    local name=""
    local watch=0
    while [[ $# -gt 0 ]]; do
    case "$1" in
        -w)
            watch=1
            ;;
        *)
            name="${1}"
        ;;
    esac
    shift
    done
    if ! [[ $name ]]; then
        echo "ERROR: No job name provided."
        return 1
    fi

    if [[ $watch ]]; then
        kubectl logs -f $name
    else
        kubectl logs $name
    fi
}

function dlda() {
    echo "Are you sure? Do you want to delete these jobs:"
    kubectl get pods -o wide
    select yn in "Yes" "No"; do
    case $yn in
        Yes ) kubectl delete --all pods ; return ;;
        No ) return ;;
    esac
    done
}

function dld() {
    local name=""
    if [[ "${1}" ]]; then
        name="${1}"
        name="${name//-master-0/}"
    else
        echo "ERROR: No job name provided."
        return 1
    fi
    kubectl delete PyTorchJob $name
}

function dli() {
    local name=""
    if [[ "${1}" ]]; then
        name="${1}"
        name="${name//-master-0/}"
    else
        echo "ERROR: No job name provided."
        return 1
    fi
    kubectl describe pod $name
}

function dls() {
    watch=0 
    while [[ $# -gt 0 ]]; do
    case "$1" in
        -w)
            watch=1
            ;;
    esac
    shift
    done

    # Show all running kubectl jobs (either once or live)
    if [[ "$watch" = 1 ]]; then
        echo "INFO: Interrupt with Ctrl+C."
        kubectl get pods -w -o wide
    else
        kubectl get pods -o wide
    fi
}

function dlr() {
    # Create manifest file, Sync 
    # For the new compute cluster the interface changed.
    local dl_user=$USER
    local dl_host=dlcluster1scratch.iis.fhg.de
    local dl_name=""

    local dl_nodes=1
    local dl_gpus=1
    local dl_cpus=8
    local dl_memory=12G

    local dl_image=pytorch/pytorch:2.2.1-cuda12.1-cudnn8-devel
    local dl_accelerator=Tesla-P100-SXM2-16GB
    # nvidia.com/gpu.product=Tesla-P40
    # nvidia.com/gpu.product=Tesla-P100-SXM2-16GB
    local dl_script=run.sh

    while [[ $# -gt 0 ]]; do
    case "$1" in
        --user*|-u*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_user="${1#*=}"
            ;;

        --host*|-s*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_host="${1#*=}"
            ;;

        --nodes*|-n*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_nodes="${1#*=}"
            ;;

        --gpus*|-g*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_gpus="${1#*=}"
            ;;

        --cpus*|-c*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_cpus="${1#*=}"
            ;;

        --memory*|-m*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_memory="${1#*=}"
            ;;

        --image*|-i*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_image="${1#*=}"
            ;;

        --accelerator*|-i*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_accelerator="${1#*=}"
            ;;

        --script*|-i*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_script="${1#*=}"
            ;;
        *)
            dl_script="${1}"
        ;;

    esac
    shift
    done

    local dl_scope_home="/scratch/home/$dl_user"
    local dl_scope_data="/scratch/scratch/data/$dl_user"
    local dl_scope_output="/scratch/scratch/output/$dl_user"

    local container_scope_home="/home/$dl_user"
    local container_scope_data="/data/$dl_user"
    local container_scope_output="/output/$dl_user"

    local dl_project=${PWD##*/}
    local dl_name="$dl_project"-"$(date -u +"%Y-%m-%d-%H-%M-%S")"

    local dl_scope_project=$dl_scope_home/projects/$dl_name/
    local container_scope_project=$container_scope_home/projects/$dl_name

    local dl_uid=$(ssh $dl_user@$dl_host 'id -u $dl_user')
    local dl_gid=$(ssh $dl_user@$dl_host 'id -g $dl_user')
    ssh $dl_user@$dl_host "ln -sf $dl_scope_data data; ln -sf $dl_scope_output output; mkdir -p $dl_scope_home/projects"

    echo "Creating manifest with configuration: "
    echo "==================================================================="
    echo "USER: $dl_user [UID: $dl_uid / GID: $dl_gid]"
    echo "MOUNTS:"
    echo "  - $dl_scope_home -> $container_scope_home"
    echo "  - $dl_scope_data -> $container_scope_data"
    echo "  - $dl_scope_output -> $container_scope_output"
    echo "HOST: $dl_host"
    echo "NODES: $dl_nodes"
    echo "CPUS: $dl_cpus"
    echo "GPUS: $dl_gpus"
    echo "MEMORY: $dl_memory"
    echo "==================================================================="

    # Syncing local project to deep learning cluster
    local git_repo="$(git rev-parse --is-inside-work-tree 2>/dev/null)"
    if [[ "$git_repo" ]]; then
        if [[ $(git rev-parse --show-toplevel) != $(pwd) ]]; then
            echo "ERROR: Not in git root directory"
            return 1
        fi
    else
        echo "ERROR: Folder is no git repository. Stop syncing..."
        return 1
    fi
    echo "INFO: Synced ${PWD} to $dl_user"@"$dl_host:$dl_scope_home/projects/$dl_name"
    rsync --exclude '.git' \
          --exclude 'logs/' \
          --exclude '__pycache__' \
          -av --delete \
          -e ssh ${PWD}/ $dl_user@$dl_host:$dl_scope_home/projects/$dl_name

    # Create/Build kubectl manifest file
    local manifest="---
apiVersion: \"kubeflow.org/v1\"
kind: PyTorchJob
metadata:
  name: $dl_name
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          creationTimestamp: null
          labels:
            nvidia.com/gpu.product: $dl_accelerator
        spec:
          containers:
            - name: pytorch
              image: $dl_image
              env:
                - name: \"NUM_CPUS\"
                  value: \"$dl_cpus\"
                - name: \"NUM_GPUS\"
                  value: \"$dl_gpus\"
                - name: \"NUM_NODES\"
                  value: \"$dl_nodes\"
                - name: \"NAME\"
                  value: \"$dl_name\"
                - name: \"HOME_PATH\"
                  value: \"$container_scope_home\"
                - name: \"DATA_PATH\"
                  value: \"$container_scope_data\"
                - name: \"OUTPUT_PATH\"
                  value: \"$container_scope_output\"
                - name: \"PROJECT_PATH\"
                  value: \"$container_scope_project\"
                - name: NVIDIA_DISABLE_REQUIRE
                  value: \"true\"
              command: [\"/bin/sh\", \"-c\"]
              args:
                - apt-get -y update ;
                  echo \$NUM_CPUS ;
                  echo \$NUM_GPUS ;
                  echo \$NUM_NODES ;
                  echo \$NUM_NAME ;
                  echo \$HOME_PATH ;
                  echo \$DATA_PATH ;
                  echo \$OUTPUT_PATH ;
                  echo \$PROJECT_PATH ;
                  pwd ;
                  nvidia-smi ;
                  cp -Rf $container_scope_project/* /workspace/ ;
                  /bin/sh /workspace/$dl_script ;
                  chown -R $dl_uid:$dl_gid $container_scope_data ;
                  chown -R $dl_uid:$dl_gid $container_scope_output ;
                  rm -Rf $container_scope_project ;
              resources:
                requests:
                  cpu: $dl_cpus
                  memory: $dl_memory
                  nvidia.com/gpu: $dl_gpus
                limits:
                  cpu: $dl_cpus
                  memory: $dl_memory
                  nvidia.com/gpu: $dl_gpus
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - name: nfs-output
                mountPath: $container_scope_output
              - name: nfs-data
                mountPath: $container_scope_data
              - name: nfs-home
                mountPath: $container_scope_home
          volumes:
          - name: nfs-output
            nfs:
              server: 10.54.182.12
              path: $dl_scope_data
              readOnly: false
          - name: nfs-data
            nfs:
              server: 10.54.182.12
              path: $dl_scope_data
              readOnly: false
          - name: nfs-home
            nfs:
              server: 10.54.182.12
              path: $dl_scope_home
              readOnly: false
          - name: dshm
            emptyDir:
              medium: Memory"
    if [[ "$dl_nodes" -gt 1 ]]; then
    local manifest=$manifest"
    Worker:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          creationTimestamp: null
          labels:
            nvidia.com/gpu.product: $dl_accelerator
        spec:
          containers:
            - name: pytorch
              image: $dl_image
              env:
                - name: \"NUM_CPUS\"
                  value: \"$dl_cpus\"
                - name: \"NUM_GPUS\"
                  value: \"$dl_gpus\"
                - name: \"NUM_NODES\"
                  value: \"$dl_nodes\"
                - name: \"NAME\"
                  value: \"$dl_name\"
                - name: \"HOME_PATH\"
                  value: \"$container_scope_home\"
                - name: \"DATA_PATH\"
                  value: \"$container_scope_data\"
                - name: \"OUTPUT_PATH\"
                  value: \"$container_scope_output\"
                - name: \"PROJECT_PATH\"
                  value: \"$container_scope_project\"
                - name: NVIDIA_DISABLE_REQUIRE
                  value: \"true\"
              command: [\"/bin/sh\", \"-c\"]
              args:
                - apt-get -y update ;
                  echo \$NUM_CPUS ;
                  echo \$NUM_GPUS ;
                  echo \$NUM_NODES ;
                  echo \$NUM_NAME ;
                  echo \$HOME_PATH ;
                  echo \$DATA_PATH ;
                  echo \$OUTPUT_PATH ;
                  echo \$PROJECT_PATH ;
                  pwd ;
                  nvidia-smi ;
                  cp -Rf $container_scope_project/* /workspace/ ;
                  /bin/sh /workspace/$dl_script ;
                  chown -R $dl_uid:$dl_gid $container_scope_data ;
                  chown -R $dl_uid:$dl_gid $container_scope_output ;
              resources:
                requests:
                  cpu: $dl_cpus
                  memory: $dl_memory
                  nvidia.com/gpu: $dl_gpus
                limits:
                  cpu: $dl_cpus
                  memory: $dl_memory
                  nvidia.com/gpu: $dl_gpus
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - name: nfs-output
                mountPath: $container_scope_output
              - name: nfs-data
                mountPath: $container_scope_data
              - name: nfs-home
                mountPath: $container_scope_home
          volumes:
          - name: nfs-output
            nfs:
              server: 10.54.182.12
              path: $dl_scope_output
              readOnly: false
          - name: nfs-data
            nfs:
              server: 10.54.182.12
              path: $dl_scope_data
              readOnly: false
          - name: nfs-home
            nfs:
              server: 10.54.182.12
              path: $dl_scope_home
              readOnly: false
          - name: dshm
            emptyDir:
              medium: Memory";
    fi
    echo $manifest | kubectl create -f -
}

function dlinit() {
    local dl_user=$USER
    local dl_host=dlcluster1scratch.iis.fhg.de

    while [[ $# -gt 0 ]]; do
    case "$1" in
        --user*|-u*)
            if [[ "$1" != *=* ]]; then shift; fi
                local dl_user="${1#*=}"
            ;;
    esac
    shift
    done

    if [[ -z "$dl_user" ]] || [[ -z "$dl_host" ]]; then
        echo "ERROR: Some or all of the required parameters are empty"
        cluster_help
        return 1
    fi
    local local_ssh_directory="$HOME/.ssh"

    # Generate rsa files if it does not exist
    if [[ -f $local_ssh_file/id_rsa ]]
    then
        echo "INFO: RSA key exists on $local_ssh_directory/id_rsa, using existing file"
    else
        ssh-keygen -t rsa -f "$local_ssh_directory/id_rsa"
        echo INFO: RSA key pair generated
    fi

    echo "We need to log into $dl_host as $dl_user to set up your public key."
    cat "$local_ssh_directory/id_rsa.pub" | ssh "$dl_host" -l "$dl_user" ' [ -d ~/.ssh ] || \
                                                                mkdir -p ~/.ssh ; \
                                                                cat > ~/.ssh/KEY ; \
                                                                KEY=$(cat ~/.ssh/KEY) ; \
                                                                export KEY ; \
                                                                grep -q "$KEY" ~/.ssh/authorized_keys || \
                                                                cat ~/.ssh/KEY >> .ssh/authorized_keys ; \
                                                                chmod 700 ~/.ssh ; \
                                                                chmod 600 ~/.ssh/authorized_keys ;
                                                                rm ~/.ssh/KEY'
    id_rsa_transfer_status=$?

    if [[ $id_rsa_transfer_status -eq 0 ]]; then
        echo "INFO: SSH set up complete: [HOST: $dl_host]"
    else
        echo "ERROR: SSH setup failed."
        return 1
    fi

    # Create project directory
    ssh $dl_user@$dl_host 'mkdir projects'
    ssh $dl_user@$dl_host 'touch .hushlogin'
    ssh $dl_user@$dl_host "ln -s /scratch/scatch/data/$dl_user/ data"
    ssh $dl_user@$dl_host "ln -s /scratch/scratch/output/$dl_user/ output"
    return 0
}

